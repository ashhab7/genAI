import json
import random
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support, accuracy_score

def load_and_generate_text(
    model_path, 
    prompts=None, 
    max_length=10, 
    temperature=0.7, 
    top_p=0.9, 
    top_k=50
):
    """
    Load a fine-tuned model and generate text for given prompts
    
    Args:
        model_path (str): Path to the saved model
        prompts (list): List of prompts to generate text for
        max_length (int): Maximum length of generated text
        temperature (float): Sampling temperature for text generation
        top_p (float): Nucleus sampling parameter
        top_k (int): Top-k sampling parameter
    
    Returns:
        list: Generated texts for each prompt
    """
    try:        
        # Default prompts if none provided
        if prompts is None:
            prompts = [
                "Who is Leonardo Da Vinci?",
                "What is the meaning of life?",
                "Explain quantum physics in simple terms."
            ]
        
        # Create text generation pipeline
        pipe = pipeline(
            task="text-generation", 
            model=model_path, 
            tokenizer=model_path,
            max_length=max_length,
            temperature=temperature,
            device="cuda:0",
            truncation=True,
            top_p=top_p,
            top_k=top_k
        )
        
        # Generate text for each prompt
        results = []
        for prompt in prompts:
            full_prompt = f"<s>[INST] {prompt} [/INST]"
            result = pipe(full_prompt)
            
            # Extract and store generated text
            if result and len(result) > 0:
                generated_text = result[0]['generated_text']
                results.append({
                    'prompt': prompt,
                    'generated_text': generated_text
                })
            else:
                results.append({
                    'prompt': prompt,
                    'generated_text': "Failed to generate text."
                })
        
        return results
    
    except Exception as e:
        print(f"Error in text generation: {e}")
        return []


# Function to extract random inputs from a JSONL file
def extract_random_inputs(jsonl_file_path, num_samples):
    inputs = []
    pairs = []

    # Read the JSONL file and collect all "input" fields
    with open(jsonl_file_path, 'r') as file:
        for line in file:
            data = json.loads(line.strip())
            if "input" in data:
                pairs.append(data["input"])
                pairs.append(data["output"]["violation"])
                inputs.append(pairs)
                pairs = []


    # Select random samples
    if len(inputs) < num_samples:
        print(f"Warning: File contains fewer than {num_samples} entries. Returning all available inputs.")
        return inputs

    random_inputs = random.sample(inputs, num_samples)
    return random_inputs

# Function to parse model prediction from generated text
def parse_model_prediction(generated_text):
    """
    Parse the model's generated text to extract whether it predicted a violation.
    
    Args:
        generated_text (str): The text generated by the model
        
    Returns:
        bool: True if the model predicted a violation, False otherwise
    """
    # Extract just the response part (after [/INST])
    if "[/INST]" in generated_text:
        text = generated_text.split("[/INST]")[1].strip()
    else:
        text = generated_text
    
    # Convert to lowercase for case-insensitive matching
    text_lower = text.lower()
    
    # Look for explicit statements about violations
    if "violation is true" in text_lower or "violation: true" in text_lower:
        return True
    elif "violation is false" in text_lower or "violation: false" in text_lower:
        return False
    
    # More flexible pattern matching
    violation_patterns = ["violat", "break", "against", "not allowed", "prohibited"]
    non_violation_patterns = ["no violation", "does not violate", "doesn't violate", "not a violation"]
    
    # Check for non-violation patterns first (they're more specific)
    for pattern in non_violation_patterns:
        if pattern in text_lower:
            return False
    
    # Then check for violation patterns
    for pattern in violation_patterns:
        if pattern in text_lower:
            return True
    
    # Default to assuming no violation if unclear
    print(f"Warning: Could not clearly determine prediction from text. Defaulting to False.")
    print(f"Text excerpt: {text[:100]}...")
    return False

# Function to calculate classification metrics
def calculate_classification_metrics(y_true, y_pred):
    """
    Calculate TP, TN, FP, FN and other metrics from ground truth and predictions.
    
    Args:
        y_true (list): Ground truth values (True/False)
        y_pred (list): Predicted values (True/False)
        
    Returns:
        dict: Dictionary containing various metrics
    """
    # Convert boolean lists to numpy arrays of 0 and 1
    y_true_np = np.array([1 if val == "true" or val is True else 0 for val in y_true])
    y_pred_np = np.array([1 if val is True else 0 for val in y_pred])
    
    # Generate confusion matrix
    cm = confusion_matrix(y_true_np, y_pred_np)
    
    # Extract values from confusion matrix
    if len(cm) == 2:  # If we have both positive and negative predictions
        tn, fp, fn, tp = cm.ravel()
    else:  # Handle the case where there's only one class in predictions
        if y_true_np[0] == 1:  # If ground truth is all positives
            tp = sum(y_pred_np)
            fn = len(y_true) - tp
            tn, fp = 0, 0
        else:  # If ground truth is all negatives
            tn = sum(1 - y_pred_np)
            fp = len(y_true) - tn
            tp, fn = 0, 0
    
    # Calculate metrics
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    # Generate classification report
    class_report = classification_report(
        y_true_np, y_pred_np,
        target_names=["No Violation", "Violation"],
        output_dict=True
    )
    
    return {
        "true_positives": tp,
        "true_negatives": tn,
        "false_positives": fp,
        "false_negatives": fn,
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1,
        "confusion_matrix": cm,
        "classification_report": class_report
    }

def main():
    # Path to your fine-tuned model
    model_path = "./llama-2-7b-chat-violation-checker"
        
    # Specify the path to your JSONL file and number of samples
    jsonl_file_path = "/scratch/ftm2nu/sentiment_analysis/support_converted.jsonl"  # Replace with your file path
    num_samples = 30

    # Extract random inputs
    random_inputs_list = extract_random_inputs(jsonl_file_path, num_samples)
    custom_prompts = []
    ground_truth = []
        
    for i in random_inputs_list:
        g = i[0]
        ground_truth.append(i[1])
        custom_prompts.append(f"If the given sentence ```{g}``` is posted in any social media, will it violate any rule? If violation is true then which rule is violated and what is the explanation. Your answer must strictly include if violation is true or false.")

    results = load_and_generate_text(
        model_path, 
        prompts=custom_prompts, 
        max_length=300  # Increased max length for more detailed responses
    )

    # Extract predictions from generated text
    predictions = []
    for result in results:
        prediction = parse_model_prediction(result['generated_text'])
        predictions.append(prediction)

    # Print results
    d = 0
    for result in results:
        print("\n--- Prompt ---")
        print(result['prompt'])
        print("\n--- Generated Text ---")
        print(result['generated_text'])
        print("-" * 50)
        print(f"True value of violation  = {ground_truth[d]}")
        print(f"Predicted value = {predictions[d]}")
        d=d+1
    
    # Calculate and display classification metrics
    print("\n" + "=" * 30 + " CLASSIFICATION METRICS " + "=" * 30)
    metrics = calculate_classification_metrics(ground_truth, predictions)
    
    print("\n--- Confusion Matrix ---")
    print(f"True Positives (TP): {metrics['true_positives']}")
    print(f"True Negatives (TN): {metrics['true_negatives']}")
    print(f"False Positives (FP): {metrics['false_positives']}")
    print(f"False Negatives (FN): {metrics['false_negatives']}")
    
    print("\n--- Performance Metrics ---")
    print(f"Accuracy: {metrics['accuracy']:.4f}")
    print(f"Precision: {metrics['precision']:.4f}")
    print(f"Recall: {metrics['recall']:.4f}")
    print(f"F1 Score: {metrics['f1_score']:.4f}")
    
    print("\n--- Classification Report ---")
    report = metrics["classification_report"]
    for class_name in ["No Violation", "Violation"]:
        if class_name in report:
            print(f"{class_name}:")
            print(f"  Precision: {report[class_name]['precision']:.4f}")
            print(f"  Recall: {report[class_name]['recall']:.4f}")
            print(f"  F1-score: {report[class_name]['f1-score']:.4f}")
            print(f"  Support: {report[class_name]['support']}")
    
    if "accuracy" in report:
        print(f"\nOverall Accuracy: {report['accuracy']:.4f}")
    if "macro avg" in report:
        print(f"Macro Avg F1: {report['macro avg']['f1-score']:.4f}")
    if "weighted avg" in report:
        print(f"Weighted Avg F1: {report['weighted avg']['f1-score']:.4f}")

if __name__ == "__main__":
    main()